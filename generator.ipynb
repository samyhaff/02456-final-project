{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "AX-wHKquKy14"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "640xxMBnXBQ8"
      },
      "outputs": [],
      "source": [
        "from pydrive.drive import GoogleDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "link = 'https://drive.google.com/file/d/1k7k6Hgyxbv1ecypwZGB4BKDxuDjMMBRY/view?usp=sharing'\n",
        "id = link.split(\"/\")[-2]\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('askhistorians.json') \n",
        "f = open('train_asks.json')\n",
        "train_data = json.load(f)\n",
        "f = open('valid_asks.json')\n",
        "valid_data = json.load(f)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "! pip install datasets\n",
        "import datasets\n",
        "\n",
        "eli5 = datasets.load_from_disk('/content/drive/MyDrive/Deep/eli5_dataset')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers\n",
        "! pip install datasets\n",
        "! pip install nlp"
      ],
      "metadata": {
        "id": "XR75zdD9e_RX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "import math\n",
        "import os  # noqa: F401\n",
        "from random import choice, randint\n",
        "from time import time\n",
        "\n",
        "import datasets  # noqa: F401\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.utils.checkpoint as checkpoint\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
        "from tqdm import tqdm\n",
        "from transformers import AdamW, AutoModel, AutoModelForSeq2SeqLM, AutoTokenizer, get_linear_schedule_with_warmup"
      ],
      "metadata": {
        "id": "uDa8Vh5UJTcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ELI5DatasetS2S(Dataset):\n",
        "    def __init__(\n",
        "        self, examples_array, make_doc_fun=None, extra_answer_threshold=3, document_cache=None, training=True\n",
        "    ):\n",
        "        self.training = training\n",
        "        self.data = examples_array\n",
        "        self.make_doc_function = make_doc_fun\n",
        "        self.document_cache = {} if document_cache is None else document_cache\n",
        "        assert not (make_doc_fun is None and document_cache is None)\n",
        "        # make index of specific question-answer pairs from multi-answers\n",
        "        if self.training:\n",
        "            self.qa_id_list = [\n",
        "                (i, j)\n",
        "                for i, qa in enumerate(self.data)\n",
        "                for j, (a, sc) in enumerate(zip(qa[\"answers\"][\"text\"], qa[\"answers\"][\"score\"]))\n",
        "                if j == 0 or sc >= extra_answer_threshold\n",
        "            ]\n",
        "        else:\n",
        "            self.qa_id_list = [(i, 0) for i in range(self.data.num_rows)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.qa_id_list)\n",
        "    def make_example(self, idx):\n",
        "        i, j = self.qa_id_list[idx]\n",
        "        example = self.data[i]\n",
        "        question = example[\"title\"] + \" \" + example[\"selftext\"]\n",
        "        answer = example[\"answers\"][\"text\"][j]\n",
        "        q_id = example[\"q_id\"]\n",
        "        if self.make_doc_function is not None:\n",
        "            self.document_cache[q_id] = self.document_cache.get(q_id, self.make_doc_function(example[\"title\"]))\n",
        "        document = self.document_cache[q_id]\n",
        "        in_st = \"question: {} context: {}\".format(\n",
        "            question.lower().replace(\" --t--\", \"\").strip(),\n",
        "            document.lower().strip(),\n",
        "        )\n",
        "        out_st = answer\n",
        "        return (in_st, out_st)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.make_example(idx)\n",
        "\n",
        "def make_qa_s2s_model(model_name=\"facebook/bart-large\", from_file=None, device=\"cuda:0\"):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
        "    if from_file is not None:\n",
        "        param_dict = torch.load(from_file)  # has model weights, optimizer, and scheduler states\n",
        "        model.load_state_dict(param_dict[\"model\"])\n",
        "    return tokenizer, model\n",
        "\n",
        "\n",
        "def make_qa_s2s_batch(qa_list, tokenizer, max_len=64, max_a_len=360, device=\"cuda:0\"):\n",
        "    q_ls = [q for q, a in qa_list]\n",
        "    a_ls = [a for q, a in qa_list]\n",
        "    q_toks = tokenizer(q_ls, max_length=max_len, padding=\"max_length\", truncation=True)\n",
        "    q_ids, q_mask = (\n",
        "        torch.LongTensor(q_toks[\"input_ids\"]).to(device),\n",
        "        torch.LongTensor(q_toks[\"attention_mask\"]).to(device),\n",
        "    )\n",
        "    a_toks = tokenizer(a_ls, max_length=min(max_len, max_a_len), padding=\"max_length\", truncation=True)\n",
        "    a_ids, a_mask = (\n",
        "        torch.LongTensor(a_toks[\"input_ids\"]).to(device),\n",
        "        torch.LongTensor(a_toks[\"attention_mask\"]).to(device),\n",
        "    )\n",
        "    labels = a_ids[:, 1:].contiguous().clone()\n",
        "    labels[a_mask[:, 1:].contiguous() == 0] = -100\n",
        "    model_inputs = {\n",
        "        \"input_ids\": q_ids,\n",
        "        \"attention_mask\": q_mask,\n",
        "        \"decoder_input_ids\": a_ids[:, :-1].contiguous(),\n",
        "        \"labels\": labels,\n",
        "    }\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "def train_qa_s2s_epoch(model, dataset, tokenizer, optimizer, scheduler, args, e=0, curriculum=False):\n",
        "    model.train()\n",
        "    # make iterator\n",
        "    if curriculum:\n",
        "        train_sampler = SequentialSampler(dataset)\n",
        "    else:\n",
        "        train_sampler = RandomSampler(dataset)\n",
        "    model_collate_fn = functools.partial(\n",
        "        make_qa_s2s_batch, tokenizer=tokenizer, max_len=args.max_length, device=\"cuda:0\"\n",
        "    )\n",
        "    data_loader = DataLoader(dataset, batch_size=args.batch_size, sampler=train_sampler, collate_fn=model_collate_fn)\n",
        "    epoch_iterator = tqdm(data_loader, desc=\"Iteration\", disable=True)\n",
        "    # accumulate loss since last print\n",
        "    loc_steps = 0\n",
        "    loc_loss = 0.0\n",
        "    st_time = time()\n",
        "    for step, batch_inputs in enumerate(epoch_iterator):\n",
        "        pre_loss = model(**batch_inputs)[0]\n",
        "        loss = pre_loss.sum() / 1 #pre_loss.shape[0]\n",
        "        loss.backward()\n",
        "        # optimizer\n",
        "        if step % args.backward_freq == 0:\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            model.zero_grad()\n",
        "        # some printing within the epoch\n",
        "        loc_loss += loss.item()\n",
        "        loc_steps += 1\n",
        "        if step % args.print_freq == 0 or step == 1:\n",
        "            print(\n",
        "                \"{:2d} {:5d} of {:5d} \\t L: {:.3f} \\t -- {:.3f}\".format(\n",
        "                    e,\n",
        "                    step,\n",
        "                    len(dataset) // args.batch_size,\n",
        "                    loc_loss / loc_steps,\n",
        "                    time() - st_time,\n",
        "                )\n",
        "            )\n",
        "            loc_loss = 0\n",
        "            loc_steps = 0\n",
        "\n",
        "\n",
        "def eval_qa_s2s_epoch(model, dataset, tokenizer, args):\n",
        "    model.eval()\n",
        "    # make iterator\n",
        "    train_sampler = SequentialSampler(dataset)\n",
        "    model_collate_fn = functools.partial(\n",
        "        make_qa_s2s_batch, tokenizer=tokenizer, max_len=args.max_length, device=\"cuda:0\"\n",
        "    )\n",
        "    data_loader = DataLoader(dataset, batch_size=args.batch_size, sampler=train_sampler, collate_fn=model_collate_fn)\n",
        "    epoch_iterator = tqdm(data_loader, desc=\"Iteration\", disable=True)\n",
        "    # accumulate loss since last print\n",
        "    loc_steps = 0\n",
        "    loc_loss = 0.0\n",
        "    st_time = time()\n",
        "    with torch.no_grad():\n",
        "        for step, batch_inputs in enumerate(epoch_iterator):\n",
        "            pre_loss = model(**batch_inputs)[0]\n",
        "            loss = pre_loss.sum() / 1#pre_loss.shape[0]\n",
        "            loc_loss += loss.item()\n",
        "            loc_steps += 1\n",
        "            if step % args.print_freq == 0:\n",
        "                print(\n",
        "                    \"{:5d} of {:5d} \\t L: {:.3f} \\t -- {:.3f}\".format(\n",
        "                        step,\n",
        "                        len(dataset) // args.batch_size,\n",
        "                        loc_loss / loc_steps,\n",
        "                        time() - st_time,\n",
        "                    )\n",
        "                )\n",
        "    print(\n",
        "        \"Total \\t L: {:.3f} \\t -- {:.3f}\".format(\n",
        "            loc_loss / loc_steps,\n",
        "            time() - st_time,\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "def train_qa_s2s(qa_s2s_model, qa_s2s_tokenizer, s2s_train_dset, s2s_valid_dset, s2s_args):\n",
        "    s2s_optimizer = AdamW(qa_s2s_model.parameters(), lr=s2s_args.learning_rate, eps=1e-8)\n",
        "    s2s_scheduler = get_linear_schedule_with_warmup(\n",
        "        s2s_optimizer,\n",
        "        num_warmup_steps=400,\n",
        "        num_training_steps=(s2s_args.num_epochs + 1) * math.ceil(len(s2s_train_dset) / s2s_args.batch_size),\n",
        "    )\n",
        "    for e in range(s2s_args.num_epochs):\n",
        "        train_qa_s2s_epoch(\n",
        "            qa_s2s_model,\n",
        "            s2s_train_dset,\n",
        "            qa_s2s_tokenizer,\n",
        "            s2s_optimizer,\n",
        "            s2s_scheduler,\n",
        "            s2s_args,\n",
        "            e,\n",
        "            curriculum=(e == 0),\n",
        "        )\n",
        "        m_save_dict = {\n",
        "            \"model\": qa_s2s_model.state_dict(),\n",
        "            \"optimizer\": s2s_optimizer.state_dict(),\n",
        "            \"scheduler\": s2s_scheduler.state_dict()\n",
        "        }\n",
        "        print(\"Saving model {}\".format(s2s_args.model_save_name))\n",
        "        eval_qa_s2s_epoch(qa_s2s_model, s2s_valid_dset, qa_s2s_tokenizer, s2s_args)\n",
        "        #torch.save(m_save_dict, \"{}_{}.pth\".format(s2s_args.model_save_name, e))\n",
        "        #torch.save(m_save_dict,\"model_eli5/\")\n",
        "        qa_s2s_tokenizer.save_pretrained(\"./model_folder/tokenizer/\")\n",
        "        qa_s2s_model.module.save_pretrained(\"./model_folder/model/\")\n",
        "      \n",
        "\n",
        "# generate answer from input \"question: ... context:  ...\"\n",
        "def qa_s2s_generate(\n",
        "    question_doc,\n",
        "    qa_s2s_model,\n",
        "    qa_s2s_tokenizer,\n",
        "    num_answers=1,\n",
        "    num_beams=None,\n",
        "    min_len=64,\n",
        "    max_len=256,\n",
        "    do_sample=False,\n",
        "    temp=1.0,\n",
        "    top_p=None,\n",
        "    top_k=None,\n",
        "    max_input_length=512,\n",
        "    device=\"cuda:0\",\n",
        "):\n",
        "    model_inputs = make_qa_s2s_batch(\n",
        "        [(question_doc, \"A\")],\n",
        "        qa_s2s_tokenizer,\n",
        "        max_input_length,\n",
        "        device=device,\n",
        "    )\n",
        "    n_beams = num_answers if num_beams is None else max(num_beams, num_answers)\n",
        "    generated_ids = qa_s2s_model.generate(\n",
        "        input_ids=model_inputs[\"input_ids\"],\n",
        "        attention_mask=model_inputs[\"attention_mask\"],\n",
        "        min_length=min_len,\n",
        "        max_length=max_len,\n",
        "        do_sample=do_sample,\n",
        "        early_stopping=True,\n",
        "        num_beams=1 if do_sample else n_beams,\n",
        "        temperature=temp,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        eos_token_id=qa_s2s_tokenizer.eos_token_id,\n",
        "        no_repeat_ngram_size=3,\n",
        "        num_return_sequences=num_answers,\n",
        "        decoder_start_token_id=qa_s2s_tokenizer.bos_token_id,\n",
        "    )\n",
        "    return [qa_s2s_tokenizer.decode(ans_ids, skip_special_tokens=True).strip() for ans_ids in generated_ids]"
      ],
      "metadata": {
        "id": "E0DhYl_VJXO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ArgumentsS2S():\n",
        "    def __init__(self):\n",
        "        self.batch_size = 2\n",
        "        self.backward_freq = 16\n",
        "        self.max_length = 1024\n",
        "        self.print_freq = 100\n",
        "        self.model_save_name = \"eli5_bart_model\"\n",
        "        self.learning_rate = 2e-4\n",
        "        self.num_epochs = 1\n",
        "\n",
        "s2s_args = ArgumentsS2S()"
      ],
      "metadata": {
        "id": "6DthOXGUKVSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s2s_train_dset = ELI5DatasetS2S(eli5['train_asks'], document_cache=dict([(k, d) for k, d, src_ls in train_data]))\n",
        "s2s_valid_dset = ELI5DatasetS2S(eli5['validation_asks'], document_cache=dict([(k, d) for k, d, src_ls in valid_data]), training=False)"
      ],
      "metadata": {
        "id": "5jW2ECVVKYMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_s2s_tokenizer, pre_model = make_qa_s2s_model(\n",
        "    model_name=\"facebook/bart-large\",\n",
        "    from_file=None,\n",
        "    device=\"cpu\"\n",
        ")\n",
        "qa_s2s_model = torch.nn.DataParallel(pre_model)"
      ],
      "metadata": {
        "id": "n1ElTu9ZKd1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_qa_s2s(qa_s2s_model, qa_s2s_tokenizer, s2s_train_dset, s2s_valid_dset, s2s_args)"
      ],
      "metadata": {
        "id": "qGhYaoTzKizH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the model and generating answers"
      ],
      "metadata": {
        "id": "TLXS4jDlK2DL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM,DistilBertTokenizer, BartTokenizer\n",
        " from lfqa_utils import *\n",
        " from eli5_utils import *"
      ],
      "metadata": {
        "id": "QWSNnOzWK4oW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BartTokenizer.from_pretrained(\"./model_folder/tokenizer/\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"./model_folder/model/\")"
      ],
      "metadata": {
        "id": "BlKvPpDOK8vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "es_client = Elasticsearch(\"http://localhost:9200\")\n",
        "wiki40b_snippets = datasets.load_from_disk(\"../wiki40b/train\""
      ],
      "metadata": {
        "id": "6L073Kw6LjqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Why do ears hurt in planes?\"\n",
        "doc, res_list = query_es_index(question, es_client, index_name='wiki40b_snippets_100w')\n",
        "question_doc = \"question: {} context: {}\".format(question, doc)\n",
        "\n",
        "answer = qa_s2s_generate(\n",
        "        question_doc, model, tokenizer,\n",
        "        num_answers=2,\n",
        "        num_beams=8,\n",
        "        min_len=64,\n",
        "        max_len=256,\n",
        "        max_input_length=1024,\n",
        "        device=\"cpu\"\n",
        ")[0]\n",
        "\n",
        "answer"
      ],
      "metadata": {
        "id": "ZDf3713nLCwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "VVIfl54gNpLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = []\n",
        "reference = []\n",
        "\n",
        "# Generate answers for the full test set\n",
        "for i in range(eli5['test_asks'].num_rows):\n",
        "    # create support document with the dense index\n",
        "    question = eli5['test_asks'][i]['title']\n",
        "    doc, res_list = query_es_index(question, es_client, index_name='wiki40b_snippets_100w')\n",
        "    # concatenate question and support document into BART input\n",
        "    question_doc = \"question: {} context: {}\".format(question, doc)\n",
        "    # generate an answer with beam search\n",
        "    answer = qa_s2s_generate(\n",
        "            question_doc, qa_s2s_model, qa_s2s_tokenizer,\n",
        "            num_answers=1,\n",
        "            num_beams=8,\n",
        "            min_len=96,\n",
        "            max_len=256,\n",
        "            max_input_length=1024,\n",
        "            device=\"cuda:0\"\n",
        "    )[0]\n",
        "    predicted += [answer]\n",
        "    reference += [eli5['test_asks'][i]['answers']['text'][0]]"
      ],
      "metadata": {
        "id": "1sgvVdkPNqh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = datasets.load_metric(\"rouge\")\n",
        "scores = nlp_rouge.compute(predictions=predicted, references=reference)\n",
        "df = pd.DataFrame({\n",
        "    'rouge1': [scores['rouge1'].mid.precision, scores['rouge1'].mid.recall, scores['rouge1'].mid.fmeasure],\n",
        "    'rouge2': [scores['rouge2'].mid.precision, scores['rouge2'].mid.recall, scores['rouge2'].mid.fmeasure],\n",
        "    'rougeL': [scores['rougeL'].mid.precision, scores['rougeL'].mid.recall, scores['rougeL'].mid.fmeasure],\n",
        "}, index=[ 'P', 'R', 'F'])\n",
        "df.style.format({'rouge1': \"{:.4f}\", 'rouge2': \"{:.4f}\", 'rougeL': \"{:.4f}\"})"
      ],
      "metadata": {
        "id": "uHZ7yhyzOR49"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}